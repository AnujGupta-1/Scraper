# Greyhound Race Scraper and Dashboard

This project consists of two parts:

* **Backend API Server** (Node.js + Express + Puppeteer) â€” for scraping greyhound race data and serving CSVs via API.
* **Frontend Dashboard** (React.js + MUI) â€” for visualizing race lists, race results, and betting odds snapshots.

---

## ğŸ“š Features

* Scrapes **race list**, **race details** (odds.com.au/greyhound/), and **race results** from [The Greyhound Recorder](https://www.thegreyhoundrecorder.com.au/).
* Saves data locally under `./exports/YYYY-MM-DD/` folders.
* Full dashboard with:

  * ğŸ Race Lists
  * ğŸ“Š Race Details (Odds Snapshots)
  * ğŸ† Race Results
  * ğŸ” Scheduler Status & Manual Scrape Trigger
* Date-picker support â€” view any past dateâ€™s saved data.

---

## ğŸ“‚ Folder Structure

```
root/
â”œâ”€â”€ src/                    # Scraper scripts
â”œâ”€â”€ exports/                 # CSV files saved by date (e.g., 2025-06-07/)
â”‚    â”œâ”€â”€ greyhound-races.csv
â”‚    â”œâ”€â”€ results.csv
â”‚    â”œâ”€â”€ race-details-HH-MM.csv
â”œâ”€â”€ logs/                    # Scraper logs
â”œâ”€â”€ dashboard/               # React frontend
â”œâ”€â”€ apiServer.js              # Express API Server
â”œâ”€â”€ package.json              # Backend dependencies
â””â”€â”€ README.md                 # This file
```

---

## ğŸš€ Getting Started

### 1. Backend API (Scraper Server)

#### Install Dependencies

```bash
npm install
```

#### Run API Server

```bash
node apiServer.js
```

Runs on [http://localhost:4000](http://localhost:4000).

### 2. Frontend Dashboard

Go to the `dashboard/` folder:

```bash
cd dashboard
npm install
npm start
```

Runs on [http://localhost:3000](http://localhost:3000).

---

## ğŸ‘¥ Dashboard Overview

* **Date Picker** â€” Select race date
* **Race List** â€” View scheduled races
* **Race Details** â€” View betting odds (Win / Place) per runner
* **Results** â€” View race results
* **Merged Odds** â€” View snapshots of odds over time
* **Scheduler Status** â€” See next/last run times and errors
* **Manual Scrape** â€” Manually trigger a scrape if needed

---

## ğŸ› ï¸ API Endpoints

| Endpoint                                            | Method | Description                            |
| --------------------------------------------------- | ------ | -------------------------------------- |
| `/api/race-list?date=YYYY-MM-DD`                    | GET    | Get race list for the date             |
| `/api/results?date=YYYY-MM-DD`                      | GET    | Get race results for the date          |
| `/api/race-details-files?date=YYYY-MM-DD`           | GET    | List race-details CSVs for the date    |
| `/api/race-details?date=YYYY-MM-DD&file=<filename>` | GET    | Get race details for specific snapshot |
| `/api/race-details-merged?date=YYYY-MM-DD`          | GET    | Get merged odds data                   |
| `/api/scheduler-status`                             | GET    | Scheduler current status               |
| `/api/trigger-scrape`                               | POST   | Manually trigger scraping              |

---

## ğŸ—“ï¸ Data Storage

Each scrape saves CSVs inside the `./exports/YYYY-MM-DD/` folder:

* `greyhound-races.csv` â€” Race listing
* `results.csv` â€” Race results
* `race-details-HH-MM.csv` â€” Odds snapshot at scrape time (multiple per day)

---

## ğŸ“ Notes

* Scraping is configured to run periodically using Node Cron.
* Puppeteer runs in stealth mode with headless browser.
* Make sure you have Chrome or Chromium installed for Puppeteer.
* Local timezone is set to **Australia/Brisbane (AEST)**.

---

## ğŸ§‘â€ğŸ’» Authors

* Developed by **Anuj Gupta**
* University of the Sunshine Coast â€” ICT342 Project
